# ğŸ¯ PLANO CIRÃšRGICO: MELHORIAS DE VARREDURA NA INTERNET
## AplicaÃ§Ã£o Universal para TODOS os Tenants e Concorrentes

---

## âœ… CONFIRMAÃ‡ÃƒO INICIAL

**SIM, as mÃ©tricas estabelecidas servem para:**
- âœ… Qualquer tenant (empresa cliente)
- âœ… Qualquer concorrente
- âœ… Qualquer URL analisada
- âœ… Qualquer busca na internet

**PadrÃ£o Universal Aplicado:**
1. **Homepage First**: Sempre acessar homepage primeiro (onde hÃ¡ produtos/serviÃ§os em destaque)
2. **SERPER Ampliado**: Busca com palavras-chave expandidas e mais resultados
3. **SubpÃ¡ginas Inteligentes**: Testar mÃºltiplas variaÃ§Ãµes de URLs comuns
4. **User-Agent Real**: Simular navegador real para evitar bloqueios
5. **Limites Aumentados**: Mais caracteres extraÃ­dos (15k homepage, 10k subpÃ¡ginas)
6. **Prompts EspecÃ­ficos**: IA focada em produtos industriais, EPIs, serviÃ§os especializados

---

## ğŸ“Š FUNCTIONS IDENTIFICADAS PARA MELHORIA

### ğŸ”´ **CATEGORIA 1: EXTRAÃ‡ÃƒO DE PRODUTOS** (2 functions)
**Status:** âœ… JÃ MELHORADAS
- `scan-website-products` âœ…
- `scan-competitor-url` âœ…

---

### ğŸŸ  **CATEGORIA 2: BUSCA WEB GENÃ‰RICA** (2 functions)
**Status:** âš ï¸ PRECISAM MELHORIAS

#### 2.1. `web-search`
**Problemas Identificados:**
- âŒ NÃ£o acessa URLs retornadas (apenas retorna snippets)
- âŒ NÃ£o faz varredura de subpÃ¡ginas
- âŒ Limite padrÃ£o muito baixo (10 resultados)
- âŒ NÃ£o extrai conteÃºdo completo das pÃ¡ginas

**Melhorias Propostas:**
1. âœ… Aumentar limite padrÃ£o de 10 para 20 resultados
2. âœ… Adicionar opÃ§Ã£o para acessar homepage dos resultados
3. âœ… Extrair conteÃºdo completo das top 5 URLs
4. âœ… Adicionar palavras-chave contextuais Ã  query
5. âœ… Melhorar User-Agent

#### 2.2. `serper-search`
**Problemas Identificados:**
- âŒ Apenas retorna resultados do SERPER (sem acesso direto)
- âŒ NÃ£o faz varredura de subpÃ¡ginas
- âŒ NÃ£o extrai conteÃºdo completo

**Melhorias Propostas:**
1. âœ… Adicionar opÃ§Ã£o para acessar URLs retornadas
2. âœ… Extrair conteÃºdo completo das top 5 URLs
3. âœ… Adicionar varredura de subpÃ¡ginas comuns
4. âœ… Melhorar User-Agent

---

### ğŸŸ¡ **CATEGORIA 3: ANÃLISE PROFUNDA DE URLs** (2 functions)
**Status:** âš ï¸ PRECISAM MELHORIAS

#### 3.1. `analyze-urls-deep`
**Problemas Identificados:**
- âš ï¸ Limite de 500 caracteres muito baixo (deveria ser 10k-15k)
- âš ï¸ User-Agent muito simples ("Mozilla/5.0")
- âš ï¸ NÃ£o acessa homepage primeiro
- âš ï¸ NÃ£o faz varredura de subpÃ¡ginas

**Melhorias Propostas:**
1. âœ… Aumentar limite de extraÃ§Ã£o de 500 para 15.000 caracteres
2. âœ… Melhorar User-Agent para navegador real
3. âœ… Priorizar homepage se URL for domÃ­nio raiz
4. âœ… Adicionar varredura de subpÃ¡ginas comuns
5. âœ… Adicionar retry logic para URLs que falham

#### 3.2. `digital-intelligence-analysis`
**Problemas Identificados:**
- âš ï¸ NÃ£o detalha como busca URLs (precisa verificar implementaÃ§Ã£o)
- âš ï¸ Pode nÃ£o estar acessando homepage primeiro
- âš ï¸ Pode nÃ£o estar varrendo subpÃ¡ginas

**Melhorias Propostas:**
1. âœ… Garantir acesso Ã  homepage primeiro
2. âœ… Adicionar varredura de subpÃ¡ginas comuns
3. âœ… Aumentar limite de caracteres extraÃ­dos
4. âœ… Melhorar User-Agent

---

### ğŸŸ¢ **CATEGORIA 4: BUSCA DE CONCORRENTES** (1 function)
**Status:** âš ï¸ PRECISA MELHORIAS

#### 4.1. `search-competitors-web`
**Problemas Identificados:**
- âš ï¸ Busca apenas em portais de comparaÃ§Ã£o
- âš ï¸ NÃ£o acessa websites dos concorrentes diretamente
- âš ï¸ NÃ£o extrai produtos/serviÃ§os dos concorrentes
- âš ï¸ Limite de 10 resultados por query

**Melhorias Propostas:**
1. âœ… Adicionar busca direta nos websites dos concorrentes
2. âœ… Acessar homepage de cada concorrente encontrado
3. âœ… Extrair produtos/serviÃ§os usando mesmo padrÃ£o de `scan-competitor-url`
4. âœ… Aumentar limite de resultados
5. âœ… Adicionar varredura de subpÃ¡ginas

---

### ğŸ”µ **CATEGORIA 5: ENRIQUECIMENTO 360Â°** (1 function)
**Status:** âš ï¸ PRECISA MELHORIAS

#### 5.1. `enrich-company-360`
**Problemas Identificados:**
- âš ï¸ FunÃ§Ã£o muito grande (1000+ linhas)
- âš ï¸ Pode nÃ£o estar acessando homepage primeiro
- âš ï¸ Pode nÃ£o estar varrendo subpÃ¡ginas
- âš ï¸ Preciso verificar implementaÃ§Ã£o de web scraping

**Melhorias Propostas:**
1. âœ… Verificar se acessa homepage primeiro
2. âœ… Adicionar varredura de subpÃ¡ginas se nÃ£o tiver
3. âœ… Aumentar limites de extraÃ§Ã£o
4. âœ… Melhorar User-Agent

---

### ğŸŸ£ **CATEGORIA 6: GERAÃ‡ÃƒO DE RELATÃ“RIOS ICP** (2 functions)
**Status:** âš ï¸ PRECISAM MELHORIAS

#### 6.1. `generate-icp-report`
**Problemas Identificados:**
- âš ï¸ Usa SERPER mas nÃ£o acessa URLs retornadas
- âš ï¸ NÃ£o extrai conteÃºdo completo
- âš ï¸ NÃ£o varre subpÃ¡ginas do website do tenant

**Melhorias Propostas:**
1. âœ… Acessar homepage do tenant se tiver website
2. âœ… Extrair conteÃºdo completo (15k caracteres)
3. âœ… Varrer subpÃ¡ginas comuns do tenant
4. âœ… Melhorar User-Agent

#### 6.2. `analyze-onboarding-icp`
**Problemas Identificados:**
- âš ï¸ Usa SERPER mas nÃ£o acessa URLs retornadas
- âš ï¸ NÃ£o extrai conteÃºdo completo
- âš ï¸ NÃ£o varre subpÃ¡ginas

**Melhorias Propostas:**
1. âœ… Acessar URLs retornadas pelo SERPER
2. âœ… Extrair conteÃºdo completo (15k caracteres)
3. âœ… Adicionar varredura de subpÃ¡ginas
4. âœ… Melhorar User-Agent

---

## ğŸ› ï¸ PADRÃƒO DE MELHORIAS UNIVERSAL

### **Template de Melhorias para TODAS as Functions:**

```typescript
// 1. SEMPRE ACESSAR HOMEPAGE PRIMEIRO
const baseUrl = url.startsWith('http') ? url : `https://${url}`;
const homepageResponse = await fetch(baseUrl, {
  headers: { 
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36' 
  },
});

// 2. EXTRAIR CONTEÃšDO COMPLETO (15k caracteres)
const textContent = html
  .replace(/<script[^>]*>[\s\S]*?<\/script>/gi, '')
  .replace(/<style[^>]*>[\s\S]*?<\/style>/gi, '')
  .replace(/<[^>]+>/g, ' ')
  .replace(/\s+/g, ' ')
  .trim()
  .substring(0, 15000);

// 3. VARRER SUBPÃGINAS COMUNS
const commonPages = [
  '/produtos', '/servicos', '/solucoes', '/catalogo',
  '/products', '/services', '/linha-produtos', '/nossos-produtos'
];

// 4. SERPER COM PALAVRAS-CHAVE EXPANDIDAS
q: `site:${domain} (produtos OR serviÃ§os OR catÃ¡logo OR soluÃ§Ãµes OR linha OR equipamentos OR EPI OR produtos em destaque)`

// 5. LIMITES AUMENTADOS
num: 15, // Aumentado de 10 para 15
max_tokens: 6000, // Aumentado de 4000 para 6000
temperature: 0.2, // Reduzido de 0.3 para 0.2 (mais preciso)
```

---

## ğŸ“‹ PLANO DE EXECUÃ‡ÃƒO POR PRIORIDADE

### **FASE 1: CRÃTICO** (Impacto Alto, EsforÃ§o Baixo)
1. âœ… `scan-website-products` - **CONCLUÃDO**
2. âœ… `scan-competitor-url` - **CONCLUÃDO**
3. ğŸ”„ `web-search` - Adicionar acesso a URLs e extraÃ§Ã£o completa
4. ğŸ”„ `serper-search` - Adicionar acesso a URLs e extraÃ§Ã£o completa

### **FASE 2: ALTA PRIORIDADE** (Impacto Alto, EsforÃ§o MÃ©dio)
5. ğŸ”„ `analyze-urls-deep` - Aumentar limites e melhorar varredura
6. ğŸ”„ `digital-intelligence-analysis` - Garantir homepage first e subpÃ¡ginas
7. ğŸ”„ `search-competitors-web` - Adicionar acesso direto a websites

### **FASE 3: MÃ‰DIA PRIORIDADE** (Impacto MÃ©dio, EsforÃ§o MÃ©dio)
8. ğŸ”„ `generate-icp-report` - Melhorar extraÃ§Ã£o de conteÃºdo
9. ğŸ”„ `analyze-onboarding-icp` - Melhorar extraÃ§Ã£o de conteÃºdo
10. ğŸ”„ `enrich-company-360` - Verificar e melhorar web scraping

---

## ğŸ¯ MÃ‰TRICAS DE SUCESSO

**Antes das Melhorias:**
- âŒ Homepage nÃ£o acessada: 0% de cobertura
- âŒ SubpÃ¡ginas nÃ£o varridas: 0% de cobertura
- âŒ Limite de caracteres: 500-5.000
- âŒ Produtos encontrados: 0-2 por site

**Depois das Melhorias:**
- âœ… Homepage sempre acessada: 100% de cobertura
- âœ… SubpÃ¡ginas varridas: 8-10 pÃ¡ginas comuns testadas
- âœ… Limite de caracteres: 15.000 (homepage) + 10.000 (subpÃ¡ginas)
- âœ… Produtos encontrados: 5-20+ por site (dependendo do site)

---

## âš ï¸ RISCOS E MITIGAÃ‡Ã•ES

**Risco 1: Rate Limiting**
- **MitigaÃ§Ã£o:** Adicionar delays entre requisiÃ§Ãµes (500ms-1s)
- **MitigaÃ§Ã£o:** Implementar retry logic com backoff exponencial

**Risco 2: Timeout em URLs Lentas**
- **MitigaÃ§Ã£o:** Timeout de 10s por URL
- **MitigaÃ§Ã£o:** Processar em paralelo com limite de 5-10 simultÃ¢neas

**Risco 3: Bloqueio por User-Agent**
- **MitigaÃ§Ã£o:** User-Agent real e rotativo
- **MitigaÃ§Ã£o:** Adicionar headers adicionais (Accept, Accept-Language)

**Risco 4: Custo de API (OpenAI/SERPER)**
- **MitigaÃ§Ã£o:** Cache de resultados por 24h
- **MitigaÃ§Ã£o:** Processar apenas top 5-10 URLs mais relevantes

---

## âœ… CHECKLIST DE VALIDAÃ‡ÃƒO

Para cada function melhorada, validar:
- [ ] Homepage Ã© acessada primeiro
- [ ] SubpÃ¡ginas comuns sÃ£o varridas (8-10 pÃ¡ginas)
- [ ] Limite de caracteres Ã© 15k (homepage) e 10k (subpÃ¡ginas)
- [ ] User-Agent Ã© real e completo
- [ ] SERPER usa palavras-chave expandidas
- [ ] Limites de resultados aumentados (15+)
- [ ] Retry logic implementado
- [ ] Timeout configurado (10s)
- [ ] Erros sÃ£o tratados graciosamente

---

## ğŸš€ PRÃ“XIMOS PASSOS

1. **AprovaÃ§Ã£o do Plano** - Aguardar confirmaÃ§Ã£o do usuÃ¡rio
2. **ImplementaÃ§Ã£o Fase 1** - ComeÃ§ar pelas functions crÃ­ticas
3. **Testes** - Validar com sites reais (ex: uniluvas.com.br)
4. **Deploy** - Fazer deploy gradual por function
5. **Monitoramento** - Acompanhar mÃ©tricas de sucesso

---

**Data de CriaÃ§Ã£o:** 2025-01-30
**Ãšltima AtualizaÃ§Ã£o:** 2025-01-30
**Status:** â³ Aguardando AprovaÃ§Ã£o

